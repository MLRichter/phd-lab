from torch.utils.data import DataLoader
from attr import attrs, attrib
from typing import Optional, Protocol, Iterable, Dict, Any
from pathlib import Path
from torch.nn.modules import Module
from torch.optim import Optimizer


@attrs(auto_attribs=True, slots=True, frozen=True)
class Metric(Protocol):
    """This class describes a metric that can be updated incrementally with each batch.
    """
    name: str

    @property
    def value(self) -> float:
        ...

    def update(self, y_true: Iterable, y_pred: Iterable) -> None:
        ...

    def reset(self) -> None:
        ...


class LRScheduler(Protocol):
    """
    Simple protocol for the learning rate schedulers used
    """

    def step(self) -> None:
        ...

    def load_state_dict(self, state_dict: Dict[str, Any]):
        ...

    def state_dict(self) -> Dict[str, Any]:
        ...


class DummyLRScheduler(LRScheduler):
    """
    The Default LR-Scheduler in case no scheduler is used
    """

    def load_state_dict(self, state_dict: Dict[str, Any]):
        return

    def state_dict(self) -> Dict[str, Any]:
        return {}

    def step(self) -> None:
        return


@attrs(auto_attribs=True, slots=True)
class DataBundle:
    dataset_name: str
    train_dataset: DataLoader
    test_dataset: DataLoader
    cardinality: int
    output_resolution: Optional[int]
    is_classifier: bool = True

    def dataset_descriptor(self) -> str:
        return f"{self.dataset_name}_{self.output_resolution}"


@attrs(auto_attribs=True, slots=True, frozen=True)
class OptimizerSchedulerBundle:

    optimizer: Optimizer
    scheduler: LRScheduler = attrib(factory=DummyLRScheduler)


class DatasetFactory(Protocol):

    def __call__(
            self,
            batch_size: int,
            output_size: int,
            cache_dir: str,
    ) -> DataBundle:
        """
        The Factory produces a dataset bundle consisting of a train, test set and sme metadata.

        :param output_resolution:   the resolution of the images creates by the test generator
        :param batch_size:          the batch size generated by both generators
        :param cache_dir:           the directory where the dataset is stored and (maybe) downloaded.
        :return: a dataset bundle consisting of training and test set.
        """
        ...


class ModelFactory(Protocol):

    def __call__(self, num_classes: int, **kwargs) -> Module:
        """
        Creates a model

        :param num_classes: number of classes or cardinality of the model output
        :param kwargs:      a number of specific args for this factory
        :return: a neural networks as PyTorch-Module, must have a "name" attribute.
        """
        ...


class OptimizerFactory(Protocol):

    def __call__(self, model: Module, **kwargs) -> OptimizerSchedulerBundle:
        """
        Provides a OptimizerFactory-Bundle for the trainer object
        :return: bundle of optimizer and trainer
        """
        ...
