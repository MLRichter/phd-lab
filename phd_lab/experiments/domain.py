from torch.utils.data import DataLoader
from attr import attrs, attrib
from typing import Optional, Iterable, Dict, Any
from typing_extensions import Protocol
from torch.nn.modules import Module
from torch.optim.optimizer import Optimizer


@attrs(auto_attribs=True, slots=True, frozen=True)
class Metric(Protocol):
    """This class describes a metric that can be updated incrementally with each batch.
    """
    name: str

    @property
    def value(self) -> float:
        """Compute the accumulated metric value
        """
        ...

    def update(self, y_true: Iterable, y_pred: Iterable) -> None:
        """Update the the metric with additional data
        Args:
            y_true:     The ground truth
            y_pred:     The prediction
        """
        ...

    def reset(self) -> None:
        """Reset the metric accumulation
        """
        ...


class LRScheduler(Protocol):
    """Simple protocol for the learning rate schedulers used
    """

    def step(self) -> None:
        """Conducts a step (generall a epoch)
        """
        ...

    def load_state_dict(self, state_dict: Dict[str, Any]):
        """Load the state dictionary for resuming training
        Args:
            state_dict:     The state dictionary
        """
        ...

    def state_dict(self) -> Dict[str, Any]:
        """Provice the current state dict for serialization
        """
        ...


class DummyLRScheduler(LRScheduler):
    """The Default LR-Scheduler in case no scheduler is used
    """

    def load_state_dict(self, state_dict: Dict[str, Any]):
        return

    def state_dict(self) -> Dict[str, Any]:
        return {}

    def step(self) -> None:
        return


@attrs(auto_attribs=True, slots=True)
class DataBundle:
    """The data bundle object contains train and test data as well as some meta-information.
    Args:
        dataset_name:           the name of the dataset
        train_dataset:          the training dataset
        test_dataset:           the test dataset
        cardinality:            for classification this is the number of classes in other tasks the
                                dimensionality of the output.
        output_resolution:      the resolution of the images provided the test-dataset
        is_classifier:          True if the dataset is a classification problem
    """
    dataset_name: str
    train_dataset: DataLoader
    test_dataset: DataLoader
    cardinality: int
    output_resolution: Optional[int]
    is_classifier: bool = True

    def dataset_descriptor(self) -> str:
        """The descriptor of the dataset used for saving and logging
        """
        return f"{self.dataset_name}_{self.output_resolution}"


@attrs(auto_attribs=True, slots=True, frozen=True)
class OptimizerSchedulerBundle:
    """The OptimizerSchedulerBundle contains the optimizer and the linked learning rate scheduler.

    Args:
        optimizer:      The optimizer
        scheduler:      The learning rate scheduler, if not set a non-operational pass through scheduler
                        will be inserted.
    """

    optimizer: Optimizer
    scheduler: LRScheduler = attrib(factory=DummyLRScheduler)


class DatasetFactory(Protocol):

    def __call__(
            self,
            batch_size: int,
            output_size: int,
            cache_dir: str,
    ) -> DataBundle:
        """The Factory produces a dataset bundle consisting of a train, test set and sme metadata.

        Args:
            output_resolution:   the resolution of the images creates by the test generator
            batch_size:          the batch size generated by both generators
            cache_dir:           the directory where the dataset is stored and (maybe) downloaded.
        Returns:
            a dataset bundle consisting of training and test set.
        """
        ...


class ModelFactory(Protocol):

    def __call__(self, num_classes: int, **kwargs) -> Module:
        """Creates a model

        Args:
            num_classes: number of classes or cardinality of the model output
            kwargs:      a number of specific args for this factory
        Returns:
            a neural networks as PyTorch-Module, must have a "name" attribute.
        """
        ...


class OptimizerFactory(Protocol):

    def __call__(self, model: Module, **kwargs) -> OptimizerSchedulerBundle:
        """Provides a OptimizerFactory-Bundle for the trainer object
        Returns:
             bundle of optimizer and trainer
        """
        ...
